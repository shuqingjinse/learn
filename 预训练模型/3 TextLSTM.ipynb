{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据前三个字母预测第四个字母"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 3\n",
    "n_hidden = 128\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "word_dict = {n:i for i,n in enumerate(char_arr)}\n",
    "number_dict = {i:w for i,w in enumerate(char_arr)}\n",
    "n_class = len(word_dict) # 26\n",
    "seq_data = ['make', 'need', 'coal', 'word', 'love',\n",
    "           'hate', 'live', 'home', 'hash', 'star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch, target_batch = [], []\n",
    "    for seq in seq_data:\n",
    "        input = [word_dict[n] for n in seq[:-1]] # m a k 对应的编号\n",
    "        target = word_dict[seq[-1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=n_hidden) # (26, 128)\n",
    "        # input (sequence_length, batch, input_size)  input_size在此为 独热编码，也即embedding的维度\n",
    "        # output (seq_len, batch, num_directions * hidden_size) # 这里的num_directions是1，因为lstm是单向的\n",
    "        self.W = nn.Linear(n_hidden, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones([n_class]))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        input = X.transpose(0, 1) # 10, 3, 26 -> 3, 10, 26\n",
    "        hidden_state = torch.zeros(1, len(X), n_hidden) #  1, 10, 128  这里的是10是 batchsize\n",
    "        cell_state = torch.zeros(1, len(X), n_hidden)\n",
    "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "        # outputs 3, 10, 1*128\n",
    "        outputs = outputs[-1] # [batch_size, n_hidden] 只要lstm最后一层的输出\n",
    "        model = self.W(outputs) + self.b # model: [batch_size, n_class]\n",
    "        return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextLSTM()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "input_batch = torch.FloatTensor(input_batch) # 10,3,26\n",
    "target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100  epoch, cost:  tensor(0.5338, grad_fn=<NllLossBackward>)\n",
      "200  epoch, cost:  tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "300  epoch, cost:  tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "400  epoch, cost:  tensor(0.0053, grad_fn=<NllLossBackward>)\n",
      "500  epoch, cost:  tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "600  epoch, cost:  tensor(0.0019, grad_fn=<NllLossBackward>)\n",
      "700  epoch, cost:  tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "800  epoch, cost:  tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "900  epoch, cost:  tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "1000  epoch, cost:  tensor(0.0006, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(epoch+1,' epoch, cost: ', loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nam', 'thi', 'sta'] -> ['d', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "predict_data = ['name','this', 'star']\n",
    "inputs = [sen[:3] for sen in predict_data]\n",
    "\n",
    "input_batch, target_batch = make_batch(predict_data)\n",
    "input_batch = torch.FloatTensor(input_batch) # 10,3,26\n",
    "# target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(inputs, '->', [number_dict[n.item()] for n in predict.squeeze()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
