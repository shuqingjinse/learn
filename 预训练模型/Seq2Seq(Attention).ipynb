{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 5\n",
    "n_hidden = 128\n",
    "\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w:i for i,w in enumerate(word_list)}\n",
    "number_dict = {i:w for i,w in enumerate(word_list)}\n",
    "n_class = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "\n",
    "    # make tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "input_batch, output_batch, target_batch = make_batch()\n",
    "# input_batch (1,5,11)\n",
    "# output_batch (1,5,11)\n",
    "# target_batch (1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "\n",
    "        # Linear for attention\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden * 2, n_class)\n",
    "\n",
    "    def forward(self, enc_inputs, hidden, dec_inputs):\n",
    "        enc_inputs = enc_inputs.transpose(0, 1)  # enc_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "        dec_inputs = dec_inputs.transpose(0, 1)  # dec_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_outputs : [n_step, batch_size, num_directions(=1) * n_hidden], matrix F\n",
    "        # enc_hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        enc_outputs, enc_hidden = self.enc_cell(enc_inputs, hidden)\n",
    "\n",
    "        trained_attn = []\n",
    "        hidden = enc_hidden\n",
    "        n_step = len(dec_inputs)\n",
    "        model = torch.empty([n_step, 1, n_class])\n",
    "\n",
    "        for i in range(n_step):  # each time step\n",
    "            # dec_output : [n_step(=1), batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            # hidden : [num_layers(=1) * num_directions(=1), batch_size(=1), n_hidden]\n",
    "            dec_output, hidden = self.dec_cell(dec_inputs[i].unsqueeze(0), hidden)\n",
    "            attn_weights = self.get_att_weight(dec_output, enc_outputs)  # attn_weights : [1, 1, n_step]\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "\n",
    "            # matrix-matrix product of matrices [1,1,n_step] x [1,n_step,n_hidden] = [1,1,n_hidden]\n",
    "            context = attn_weights.bmm(enc_outputs.transpose(0, 1))\n",
    "            dec_output = dec_output.squeeze(0)  # dec_output : [batch_size(=1), num_directions(=1) * n_hidden]\n",
    "            context = context.squeeze(1)  # [1, num_directions(=1) * n_hidden]\n",
    "            model[i] = self.out(torch.cat((dec_output, context), 1))\n",
    "\n",
    "        # make model shape [n_step, n_class]\n",
    "        return model.transpose(0, 1).squeeze(0), trained_attn\n",
    "    def get_att_weight(self, dec_output, enc_outputs):  # get attention weight one 'dec_output' with 'enc_outputs'\n",
    "        n_step = len(enc_outputs)\n",
    "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step]\n",
    "\n",
    "        for i in range(n_step):\n",
    "            attn_scores[i] = self.get_att_score(dec_output, enc_outputs[i])\n",
    "\n",
    "        # Normalize scores to weights in range 0 to 1\n",
    "        return F.softmax(attn_scores).view(1, 1, -1)\n",
    "\n",
    "    def get_att_score(self, dec_output, enc_output):  # enc_outputs [batch_size, num_directions(=1) * n_hidden]\n",
    "        score = self.attn(enc_output)  # score : [batch_size, n_hidden]\n",
    "        return torch.dot(dec_output.view(-1), score.view(-1))  # inner product make scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1, 1, n_hidden)\n",
    "\n",
    "model = Attention()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 cost = 0.000499\n",
      "Epoch: 0800 cost = 0.000163\n",
      "Epoch: 1200 cost = 0.000081\n",
      "Epoch: 1600 cost = 0.000048\n",
      "Epoch: 2000 cost = 0.000031\n",
      "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(input_batch, hidden, output_batch)\n",
    "\n",
    "    loss = criterion(output, target_batch.squeeze(0))\n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Test\n",
    "test_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
    "test_batch = torch.FloatTensor(test_batch)\n",
    "predict, trained_attn = model(input_batch, hidden, test_batch)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "# Show Attention\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(trained_attn, cmap='viridis')\n",
    "ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
    "ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
