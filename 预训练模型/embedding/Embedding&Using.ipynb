{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用gensim训练垂直领域的字向量，并且在模型中进行简单使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的语料：\n",
      " ----------\n",
      "这 是 一 个 大 苹 果 \n",
      "这 是 一 个 大 菠 萝 \n",
      "这 是 一 个 大 香 蕉 \n",
      "那 是 一 辆 小 汽 车 \n",
      "那 是 一 架 大 飞 机 \n",
      "那 是 一 艘 小 轮 船"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "f = codecs.open('simple.txt','r',encoding='utf8')\n",
    "target = codecs.open('simple_train.txt','w',encoding='utf8')\n",
    "line_num = 1\n",
    "line = f.readline()\n",
    "print('处理后的语料：\\n','-'*10)\n",
    "while line:\n",
    "    line_seg = ' '.join(str(line))\n",
    "    target.writelines(line_seg)\n",
    "    print(line_seg,end='')\n",
    "    line_num = line_num+1\n",
    "    line = f.readline()\n",
    "f.close\n",
    "\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(LineSentence('simple_train.txt'), size=2, window=3, min_count=1)\n",
    "# size 指的是字向量的大小\n",
    "# window 指的是w2v的窗口大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('simple') # 此文件为二进制文件，无法直接打开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('simple_not_c', binary=False)\n",
    "# 此文件可以直接打开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载刚训练好的字向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_word2vec_model = Word2Vec.load('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('机', 0.9983408451080322)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_word2vec_model.most_similar('苹',topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05518538, -0.21129605], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接拿到某个字的字向量\n",
    "simple_word2vec_model.wv['苹']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [word for word, Vocab in simple_word2vec_model.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {\" \": 0}\n",
    "word_vector = {}\n",
    "embedding_matrix = np.zeros((len(vocab_list) + 1, simple_word2vec_model.vector_size))\n",
    "\n",
    "for i in range(len(vocab_list)):\n",
    "    # print(i)\n",
    "    word = vocab_list[i]  # 每个词语\n",
    "    word_index[word] = i + 1 # 词语：索引\n",
    "    word_vector[word] = simple_word2vec_model.wv[word] # 词语：词向量\n",
    "    embedding_matrix[i + 1] = simple_word2vec_model.wv[word]  # 词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape\n",
    "# 即词向量矩阵\n",
    "# word_index 为 “词语-索引”字典\n",
    "number_dict = {i:w for w,i in word_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练好的字向量用于 nnlm 模型当中\n",
    "\n",
    "值得注意的是 需要按照 word_index 进行构造输入输出的batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sen in sentences:\n",
    "        word = ' '.join(sen).split()\n",
    "        input = []\n",
    "        for n in word[:-1]:\n",
    "            if n in word_index.keys():\n",
    "                input.append(word_index[n])\n",
    "            else:\n",
    "                input.append(0)\n",
    "        target = word_index[word[-1]]\n",
    "        \n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, target_batch\n",
    "# 需要按照 word_index 构造输入输出batch\n",
    "sentences = [ \"这有一个大苹果\", \"那有一艘小轮船\", \"那是一架大飞机\"]\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "input_batch = Variable(torch.LongTensor(input_batch))\n",
    "target_batch = Variable(torch.LongTensor(target_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNLM(\n",
      "  (embedding): Embedding(23, 2)\n",
      "  (fc1): Linear(in_features=12, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=23, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_step = 6  # 考虑前两个词\n",
    "n_hidden = 2\n",
    "m = 2\n",
    "input_size = n_step * m # 2 * 2\n",
    "hidden_size = n_hidden  # 2 隐藏层单元数为 2\n",
    "n_class = 23\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        # 在embedding层中不使用预训练好的word2vec词向量\n",
    "        # self.emb = nn.Embedding(n_class, m)\n",
    "        \n",
    "        # 使用预训练词向量\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        # requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size) \n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.n_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) \n",
    "        x = x.view(-1, self.input_size) \n",
    "        x = self.fc1(x)          \n",
    "        x = torch.tanh(x)  \n",
    "        output = self.fc2(x)      \n",
    "        return output\n",
    "\n",
    "model = NNLM(input_size, hidden_size, n_class)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.779972\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19],\n",
      "        [22],\n",
      "        [19]])\n",
      "['这有一个大苹', '那有一艘小轮', '那是一架大飞'] -> ['机', '船', '机']\n"
     ]
    }
   ],
   "source": [
    "# 预测结果\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(predict)\n",
    "\n",
    "print([sen[:-1] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed in model.embedding.parameters():\n",
    "    embedding_after_train = embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "针对 \"是\" 的字向量为\n",
      " 训练后 [-0.2596454322338104, 0.33751505613327026] \n",
      " 训练前 [-0.14180711 -0.1044377 ]\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print('针对 \"%s\" 的字向量为\\n 训练后 %s \\n 训练前 %s'% (number_dict[i],embedding_after_train[i].tolist(), embedding_matrix[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用搜狗的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "embedding_pretrained = torch.tensor(np.load('embedding_SougouNews.npz')[\"embeddings\"].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "vocab_path = 'vocab.pkl'\n",
    "vocab = pkl.load(open(vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4762"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_vocab = {v:i for i,v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sen in sentences:\n",
    "        word = ' '.join(sen).split()\n",
    "        input = []\n",
    "        for n in word[:-1]:\n",
    "            input.append(vocab.get(n, vocab.get('<UNK>')))\n",
    "        target = vocab.get(word[-1], vocab.get('<UNK>'))\n",
    "        \n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, target_batch\n",
    "# 需要按照 word_index 构造输入输出batch\n",
    "sentences = [ \"这有一个大苹果\", \"那有一艘小轮船\", \"那是一架大飞机\"]\n",
    "input_batch, target_batch = make_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNLM(\n",
      "  (embedding): Embedding(4762, 300)\n",
      "  (fc1): Linear(in_features=1800, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=4762, bias=True)\n",
      ")\n",
      "Epoch: 1000 cost = 3.154887\n",
      "tensor([[ 579],\n",
      "        [1172],\n",
      "        [  47]])\n"
     ]
    }
   ],
   "source": [
    "input_batch = Variable(torch.LongTensor(input_batch))\n",
    "target_batch = Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "\n",
    "n_step = 6  # 考虑前两个词\n",
    "n_hidden = 2\n",
    "m = 300\n",
    "input_size = n_step*m # 2 * 2\n",
    "hidden_size = n_hidden  # 2 隐藏层单元数为 2\n",
    "n_class = len(vocab)\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_class):\n",
    "        super(NNLM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        # 在embedding层中不使用预训练好的word2vec词向量\n",
    "        # self.emb = nn.Embedding(n_class, m)\n",
    "        \n",
    "        # 使用预训练词向量\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_pretrained)\n",
    "        # requires_grad指定是否在训练过程中对词向量的权重进行微调\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size) \n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.n_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) \n",
    "        x = x.view(-1, self.input_size) \n",
    "        x = self.fc1(x)          \n",
    "        x = torch.tanh(x)  \n",
    "        output = self.fc2(x)      \n",
    "        return output\n",
    "\n",
    "model = NNLM(input_size, hidden_size, n_class)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "for epoch in range(1000):\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "# 预测结果\n",
    "\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这有一个大苹', '那有一艘小轮', '那是一架大飞'] -> ['果', '船', '机']\n"
     ]
    }
   ],
   "source": [
    "print([sen[:-1] for sen in sentences], '->', [index_to_vocab[n.item()] for n in predict.squeeze()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
